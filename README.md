# Deploy-llama2-on-Ubuntu-VM
How to deploy the llama2 model and initiate the chat on the Ubuntu VMware on Macbook intel

## 1. Preparation step on your Intel Macbook
find the ubuntu image on the website https://www.linuxvmimages.com/images/ubuntu-2004/, download the ubuntu newest version file for VMware.
Then use the VMware to load the image and install all the system updates using following command on terminal within Ubuntu.
"sudo apt full-upgrade -y"
Then reboot the system
"sudo reboot"

## 2. prepare the python env. for deployment

### 2.1 install Python 3
Check if Python 3 is installed:
"python3 --version"
If it‚Äôs not installed, install Python 3 and python3-pip:
"sudo apt install -y python3 python3-pip"

### 2.2 prepare virtual environment isolation for llama2 installation
To keep your project dependencies isolated, use a virtual environment. Install venv if it‚Äôs not already available:
"sudo apt install -y python3-venv"

Create a Virtual Environment
Navigate to your project directory or create a new one:
"mkdir llama2_CPU"
"cd llama2_CPU"
Create a virtual environment called llama2_CPU
"python3 -m venv llama2_CPU"
Activate the Virtual Environment
Activate the environment to install dependencies within it:
"source llama2_env/bin/activate"

## 3. Install dependencies for running LLaMA locally
llama-cpp-python is a project based on lama.cpp which allow you to run Llama models on your local Machine by 4-bits Quantization.

üëÅÔ∏è Quantization: Quantization refers to the process of reducing the precision of a model‚Äôs weights and activations from floating-point numbers to integers. This can be useful for deploying models on devices with limited computational resources, as it can reduce memory usage and improve computational efficiency.

First we go to the project directory
"cd ~/llama_CPU"

we install the library which is python friendly for python
"pip3 install llama-cpp-python"

## 4. Access the llama2 model from hugging face website "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main"
here because we use the Intel CPU based VM without independent GPU, with a minimum 8GB RAM and 8GM shared graphic memory, so we choose the most lightweighted llama2 model llama-2‚Äì7b-chat.Q2_K.gguf.

But before you can download the model from hugging face, you need to regisery an acount and get your private token, which we will need in the following curl command

About GGUF
üëÅÔ∏è GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.

go to the subfolder models
"cd ./models"

use the curl to download our chosen model from the download link (go on that hugging face website provided above and find that model, copy the download link)
"curl -L -o llama-2-7b-chat.Q2_K.gguf -H "Authorization: Bearer YOUR_TOKEN" "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf"

## 5. Test the model using jupyter notebook
after the model is downloaded, we can use jupyter notebook to test the model.

With the virtual environment activated, install Jupyter Notebook
"pip install jupyter"

To use the virtual environment as a kernel in Jupyter, install ipykernel:
"pip install ipykernel"

Use the following command to add your virtual environment to Jupyter Notebook‚Äôs list of available kernels:
"python -m ipykernel install --user --name=llama2_CPU --display-name "Python (llama2_CPU)"

Now that everything is set up, start Jupyter Notebook:
"jupyter notebook"
On the jupyter notebook interface, you can choose on the right upper corner to kernel to the one we created "llama2_CPU"

Run the following inference to test the model we installed
"from llama_cpp import Llama

''' Put the location of to the GGUF model that you've download from HuggingFace here'''
model_path = "models/llama-2-7b-chat.Q2_K.gguf"
llm = Llama(model_path=model_path)

''' Prompt creation '''
system_message = "You are a helpful assistant"
user_message = "Q: Name the planets in the solar system? A: "

prompt = f"""<s>[INST] <<SYS>>
{system_message}
<</SYS>>
{user_message} [/INST]"""

''' Run the model '''
output = llm(
  prompt, # Prompt
  max_tokens=32, # Generate up to 32 tokens
  stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
  echo=True # Echo the prompt back in the output
) # Generate a completion, can also call create_completion

print(output)"

